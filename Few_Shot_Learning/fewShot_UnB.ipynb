{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<br>\n",
    "Reference: 1 Monitor ReWiS Code by Niloofar Bahadori<br>\n",
    "ReWiS:<br>\n",
    "One Access Point Tx with 4 Antennas, One Station Rx with 3 Antennas. 12 links, 224 x 224 images only Abs (no phase)\n",
    "Single model for HAR few-shot classification (no 3 models as in majority vote)\n",
    "\n",
    "Training:<br>\n",
    "    1) Read csi_data, 2) Train the CNN embedding to find the prototypes 3) ProtoNet and Few-shot<br>\n",
    "Testing:<br>\n",
    "    1) Read csi_data, 2) Utilize the trained embeddings, and 3) ProtoNet for predition<br>\n",
    "\n",
    "Few-shot learning for classifying human activities for the case of one monitor.<br>\n",
    "This code does not require embedding training as the embedding is already trained.<br>\n",
    "To train a model, load any dataset start with m1c1_xxx or m1c4_xxx. <br>\n",
    "The code trains the model with the data collected from environment A1 and can be tested on either A2 or A3.<br>\n",
    "\n",
    "\n",
    " 7 Jul 2022 AB ITIV KIT/KARLSRUHE/de - KIT/GDH ESP32 S261 Cycle=750ms dataset\n",
    "23 Oct 2024 AB LARA/UnB/br - sala estudos/sala reuniões ESP32 S241 Cycle=400ms dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import csv\n",
    "import os.path\n",
    "import numpy as np\n",
    "import math\n",
    "import cmath\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from   torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "import torch.optim as optim\n",
    "from   torch.optim import Adam, SGD\n",
    "from   torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from   torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from   scipy.io import loadmat\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.metrics import accuracy_score\n",
    "from   sklearn.metrics import confusion_matrix\n",
    "from   scipy import signal\n",
    "from   sklearn.decomposition import PCA\n",
    "from   scipy.signal import butter,filtfilt\n",
    "from   PIL import Image\n",
    "from   scipy.io import loadmat\n",
    "from   sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import platform\n",
    "from   tqdm.notebook import trange "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p =  <multiprocessing.pool.Pool state=RUN pool_size=16>\n",
      "spawn\n",
      "Sistema operacional: Windows 10\n",
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p = mp.get_context(\"spawn\").Pool(16)\n",
    "    print('p = ', p)\n",
    "    # Exemplo de uso\n",
    "    # results = p.map(worker_function, list_of_inputs)\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    print(mp.get_start_method())\n",
    "\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "# Verificar a versão do sistema operacional\n",
    "print(\"Sistema operacional:\", platform.system(), platform.release())\n",
    "\n",
    "# Verificar se CUDA está disponível\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "<br>\n",
    " Load the dataset's folder:either m3c1_xxx or m3c4_xxx ###<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_folder =  ../fewShot_UnB/data ; test_folder =  ../fewShot_UnB/data4\n",
      "model_folder =  ../fewShot_UnB/models/model_4.pt ; result_out_name =  ../fewShot_UnB/results/result_A14_data.pt\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# Fixed CSI parameters\n",
    "##########################################################################\n",
    "NS    = 4       # Number of Sensors 241 configuration 4.feb.23 GDH\n",
    "n_way = 4       # 4 activity classes\n",
    "oAbs  = False   # only Absolute values (no Phase) - PCA in trouble with oAbs=True, NS=4\n",
    "\n",
    "##########################################################################\n",
    "# Directories - nameFile = given unique configuration of a fewShot_UnB run\n",
    "##########################################################################\n",
    "data_folder='data'\n",
    "train_env = ''\n",
    "test_env = '4'\n",
    "\n",
    "train_folder_name = '../fewShot_UnB/' + data_folder + train_env\n",
    "test_folder_name = '../fewShot_UnB/' + data_folder + test_env\n",
    "model_out_name = '../fewShot_UnB/models/model_' + test_env + '.pt'\n",
    "result_out_name = '../fewShot_UnB/results/result_A1' + test_env +'_' + data_folder + '.pt'\n",
    "\n",
    "print(\"train_folder = \", train_folder_name,\"; test_folder = \", test_folder_name)\n",
    "print(\"model_folder = \", model_out_name,\"; result_out_name = \", result_out_name)\n",
    "\n",
    "# ################################\n",
    "# META-PARAMETERS - Tuning options\n",
    "# ################################\n",
    "B10s   = 14     # [10:15] Broadcast available CSI in a 10s image (images are Amp/Phase: 2*B10s x 416)\n",
    "N10s   = NS*B10s # 52    # 10s / 0,4s/BC * 4 linCSV/BC = 100 linCSIPerFrame (25 x 416) images (\"Should be\")\n",
    "wPCA   = True  # with PCA data reduction 416 -> 26\n",
    "wFilter= True  # median Outlyer and 3rd O. Butterworth noise reduciton filter\n",
    "phaseSanit = True\n",
    "\n",
    "n_support = 4   \n",
    "n_query = 3    \n",
    "\n",
    "######################################\n",
    "trainD = True   # train Data / test Data\n",
    "NTrain=14 \n",
    "NTrain=19      # Number of Training Files (will be divided in 60% train and 40% test for the CNN base model)\n",
    "NTest=6        # Number of Test (fewShot) Files\n",
    "   \n",
    "\n",
    "if wPCA:      tPCA = 'wPCA'\n",
    "else: tPCA = 'nPCA'\n",
    "if wFilter:   tF = 'wF'\n",
    "else: tF = 'nF'\n",
    "if phaseSanit:   tS = 'wS'\n",
    "else: tS = 'nS'\n",
    "\n",
    "nameFile = 'B10s'+str(B10s)+'_'+tF+'_'+tS+'_'+tPCA+'_n'+str(n_support)+'sup'+'_n'+str(n_query)+'qry'+ '.pt'\n",
    "result_out_name = 'results/result_' + nameFile\n",
    "meta_out_name = 'results/meta_' + nameFile\n",
    "model_out_name = 'models/model_' + nameFile\n",
    "\n",
    "result = {'CF_P':[], 'acc_P':[],'CF_FS':[], 'acc_FS':[]}\n",
    "meta={'NS':NS,'N10s':N10s,'wPCA':wPCA,'wFilter':wFilter,'oAbs':oAbs,'NTrain':NTrain,'NTest':NTest,'n_way':n_way,'n_support':n_support,'n_query':n_query}\n",
    "\n",
    "##############################################################\n",
    "# useBest = False -> MAIN (non-linear CNN) PROTOTYPEs TRAINING (SLOW - train everything)\n",
    "# useBest = True -> use the best saved model (FAST - enhance only FewShot full connected layer)\n",
    "##############################################################\n",
    "useBest = False\n",
    "\n",
    "if useBest and os.path.isfile(result_out_name):                \n",
    "    result=torch.load(result_out_name)\n",
    "    meta=torch.load(meta_out_name)\n",
    "    CF_P=result['CF_P']\n",
    "    acc_P=result['CF_P']\n",
    "else: # a new model has to be created (maybe overwritting the existing)\n",
    "    useBest=False\n",
    "    CF_P=[]\n",
    "    acc_P=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# PARAMETERS for readCSI_KIT \n",
    "####################################\n",
    "\n",
    "max_epoch = 2\n",
    "epoch_size = 1000\n",
    "test_episode = 100\n",
    "\n",
    "# Without insertions 400ms give 26x26 images\n",
    "# BUT!! with about 50% of BC fill 26x26 images SO: B10s = 15 uses some of the spare CSI lines                              \n",
    "\n",
    "     \n",
    "# ################################\n",
    "# Images and Figures\n",
    "# ################################\n",
    "showIm = False   \n",
    "wEmptyFrames = False\n",
    "pltCSItiming = False\n",
    "\n",
    "CNN_Leandro = True # only train CNN (prototypes) no FewShot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# LP Noise Reduction Filter \n",
    "###########################\n",
    "\n",
    "T = 1.0         # Sample Period\n",
    "fs = 10.0       # sample rate, Hz\n",
    "cutoff = 2      # desired cutoff frequency of the filter, Hz ,      slightly higher than actual 1.2 Hz\n",
    "nyq = 0.5 * fs  # Nyquist Frequency\n",
    "order = 2       # sin wave can be approx represented as quadratic\n",
    "n = int(T * fs) # total number of samples\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order):\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#   read_csiKIT - BUILD ML data from .csv files\n",
    "# trainD: True (training data), False (test data)\n",
    "# wPCA = True  -> with PCA -> abPCA(26,26)\n",
    "#        False -> abPCA(26,416), for img visualization\n",
    "# showIm = True -> show (imgI, imR, ab, abPCA)  \n",
    "# Niter = number of .csv files readed (max. 9)  \n",
    "# wFilter = median (for outlyers) and LP (for noise)\n",
    "# oAbs = only Abs (no Phase) - (B10sx26) Images   \n",
    "# \n",
    "# trainx - CSI frames \n",
    "# trainy - frame labels \n",
    "# imgI, imgR - Img and Real CSI matrices (N10sx264)\n",
    "# ab, abPCA  - Abs/Phase (26x416), PCA compr.(26x26)\n",
    "####################################################\n",
    "\n",
    "def read_csiKIT (trainD, wPCA, showIm, Niter, wFilter, oAbs):\n",
    "\n",
    "    trainy=[]\n",
    "    trainx=[]\n",
    "    emptyTime=[] #np.array([])\n",
    "  \n",
    "  \n",
    "    iNiter = 0           \n",
    "    if trainD==True: NFrames=23    # 5 Train experiments with 23 frames each                 \n",
    "    else: NFrames=23               # 2 Test  experiment  with 23 frames \n",
    "    \n",
    "    for ex in range(iNiter,Niter):\n",
    "        \n",
    "        # considering 4 sensors in S241. \n",
    "        if NS == 4:\n",
    "            if trainD:                 # TRAIN data from \"Sala de Estudos LARA\" - Arq. por ordem de tamanho\n",
    "                if ex==0: file_in = 'cEu_rot6_06022025.csv'\n",
    "                if ex==1: file_in = 'cEu_rot1_06022025.csv'\n",
    "                if ex==2: file_in = 'cEu_rot4_06022025.csv'  \n",
    "                \n",
    "\n",
    "                if ex==3: file_in = 'cEu_rot3_27012025.csv'\n",
    "                if ex==4: file_in = 'cEu_rot5_27012025.csv'\n",
    "                if ex==5: file_in = 'cEu_rot5_27012025.csv'\n",
    "                if ex==6: file_in = 'cEu_rot4_22012025.csv'\n",
    "                if ex==7: file_in = 'cEu_rot7_01022025.csv'\n",
    "                if ex==8: file_in = 'cEu_rot2_27012025.csv' \n",
    "                if ex==9: file_in = 'cEu_rot4_27012025.csv'\n",
    "                if ex==10: file_in = 'cEu_rot2_22012025.csv'\n",
    "                if ex==11: file_in = 'cEu_rot5_01022025.csv' \n",
    "                if ex==12: file_in = 'cEu_rot3_22012025.csv'\n",
    "                if ex==13: file_in = 'cEu_rot8_01022025.csv' \n",
    "                if ex==14: file_in = 'cEu_rot1_22012025.csv'\n",
    "                if ex==15: file_in = 'cEu_rot6_27012025.csv'\n",
    "                if ex==16: file_in = 'cEu_rot6_01022025.csv' \n",
    "                \n",
    "                if ex==17: file_in = 'cEu_rot5_06022025.csv'\n",
    "                if ex==18: file_in = 'cEu_rot2_06022025.csv'\n",
    "                if ex==19: file_in = 'cEu_rot3_06022025.csv'\n",
    "                                               \n",
    "                               \n",
    "            else:    # captured at the same day, same conditions!\n",
    "                # if ex==0: file_in = 'cEu_rot5_06022025.csv'\n",
    "                # if ex==1: file_in = 'cEu_rot2_06022025.csv'\n",
    "                # if ex==2: file_in = 'cEu_rot3_06022025.csv'\n",
    "                                               \n",
    "                if ex==0: file_in = 'crot6.csv'   \n",
    "                if ex==1: file_in = 'crot1.csv'\n",
    "                if ex==2: file_in = 'crot2.csv'\n",
    "                if ex==3: file_in = 'crot3.csv'\n",
    "                if ex==4: file_in = 'crot4.csv'   \n",
    "                if ex==5: file_in = 'crot5.csv'   \n",
    "     \n",
    "                \n",
    "        file_in = 'data4/f'+file_in   # 'f' - fixed CSI files by inserting lines to match sequence pattern\n",
    "        \n",
    "        if oAbs: pca = PCA(n_components = B10s)   # only Abs\n",
    "        else:    pca = PCA(n_components = 2*B10s)   # Abs & Phase\n",
    "    \n",
    "        t = []\n",
    "        y = []\n",
    "        yv= []\n",
    "        ya= []\n",
    "        carrier = []\n",
    "        bords=[]\n",
    "        bordC=[]\n",
    "        nImg=1\n",
    "\n",
    "        with open(file_in,'r') as csvfile:\n",
    "            lines = csv.reader(csvfile, delimiter=',')\n",
    "            for row in lines:\n",
    "                t.append(float(row[13]))\n",
    "                y.append(row[16])\n",
    "                \n",
    "        # Subtrai offset de t[0] -> polarização do tempo, desloca todos os valores de CSI\n",
    "        #for indi in range(len(t)):\n",
    "        #    t[indi]-=t[0]        \n",
    "        \n",
    "        diff=0      \n",
    "        #--------------------- \n",
    "        if pltCSItiming: \n",
    "            if ex==0:\n",
    "                print('experiment ex:',ex,'len(y)',len(y),'NS',NS,'N10s',N10s,'t[ini],t[ini+52],diff,ini,int((ini+1)/52),pas,pas-ini')\n",
    "                plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        if showIm:\n",
    "            if ex==0:\n",
    "                fig, axs = plt.subplots(round(NFrames/6),6)   # \n",
    "                fig.suptitle(file_in+'CSI Amplitude (4 x 416) :> 416 subcarriers = 4 sensors*(108 -4 pilots)')\n",
    "                plt.axis('off')\n",
    "                fig.set_figwidth(10)\n",
    "                fig.set_figheight(10)\n",
    "\n",
    "                fg, ax = plt.subplots(round(NFrames/6),6)\n",
    "                fg.suptitle('CSI PCA (40 x 40) -> PCA 40 components')\n",
    "                plt.axis('off')\n",
    "                fg.set_figwidth(10)\n",
    "                fg.set_figheight(10)\n",
    "\n",
    "        ######################################\n",
    "        # find 10 sec borders\n",
    "        ######################################\n",
    "        nearest = 100\n",
    "        for idx, val in enumerate(t):\n",
    "            searchV = nImg*10000.\n",
    "            if searchV -val < nearest:      # approximating the border\n",
    "                nearest = searchV - val \n",
    "                bord=[idx, val]\n",
    "            if nearest <= 0:                # getting away from th border\n",
    "                bords.append(bord)          # Register the border (last nearest)\n",
    "                nImg+=1\n",
    "                nearest = t[idx+2]                \n",
    "        bords=np.array(bords)\n",
    "        #print('bords',bords)\n",
    "        \n",
    "        ######################################\n",
    "        # find 400 msec borders (BroadCast CYCLE)\n",
    "        ######################################\n",
    "        nearestC = 100    \n",
    "        nImg = 1\n",
    "        for idxx, valx in enumerate(t):\n",
    "            searchV = nImg*1000.\n",
    "            if searchV -valx < nearestC:      # approximating the border\n",
    "                nearestC = searchV - valx \n",
    "                bordA=[idxx, valx]\n",
    "            if nearestC <= 0:                # getting away from th border\n",
    "                bordC.append(bordA)          # Register the border (last nearest)\n",
    "                nImg+=1\n",
    "                nearestC = t[idxx+0]                \n",
    "        bordC=np.array(bordC)\n",
    "     \n",
    "        #-----------------\n",
    "        # if pltCSItiming:\n",
    "        #     if ex==0:\n",
    "        #         print('bordC',len(bordC)) #,bordC[1,1]-bordC[0,1],bordC[2,1]-bordC[1,1])\n",
    "        #         for ii in range(30):\n",
    "        #             print(bordC[ii,1]*0.001,(bordC[ii+1,1]-bordC[ii,1])*0.001)\n",
    "        #             print(bordC)\n",
    "         \n",
    "        #-------------------- \n",
    "        # diff=np.array([40])\n",
    "        # for ii in range(0,39):\n",
    "        #    diff[ii]=bordC[ii+1,1]-bordC[ii,1]\n",
    "        # print('diff',diff*0.001) \n",
    "        # print('t',np.array(t[0:40]))  \n",
    "        #print('bordC',bordC[0:40,0],bordC[0:40,1]*0.001,(bordC[0:40,1]-bordC[0,1])*0.001)\n",
    "        \n",
    "        ######################################\n",
    "        # join CSI lines from the last border, until 10 seconds frame is complete\n",
    "        for k in range (NFrames):            \n",
    "            CSi = []\n",
    "            CSr = []\n",
    "            var = []\n",
    "            \n",
    "            if k == 0: ini = 0\n",
    "            else : ini = int(bords[k-1,0])\n",
    "            pas = int(bords[k,0])\n",
    "            \n",
    "            diff = int(t[pas] -int(t[ini+52]))\n",
    "            ######################################\n",
    "            #while t(n) in next 10s window\n",
    "            #-----------------------------\n",
    "            #print('t[ini], t[ini+52],ini,(ini+1)/52,pas,pas-ini',int(t[ini]),int(t[ini+52]),diff,ini,int((ini+1)/52),pas,pas-ini)\n",
    "            \n",
    "            #--------------\n",
    "            if pltCSItiming:\n",
    "                if ex==0:\n",
    "                    ttt=np.array(t)\n",
    "                    xtt=[]\n",
    "                    xtt=range(ini,ini+52)\n",
    "                    if k<2:\n",
    "                        plt.plot(xtt,ttt[ini:ini+52]*0.001,'*')\n",
    "\n",
    "\n",
    "            # take the first 52 CSI lines\n",
    "            for j in range (ini, ini+N10s, NS):         # One 10 seconds Frame\n",
    "                cvv=[]\n",
    "                for i in range(NS):            \n",
    "                    cvv.append([float(item) for item in y[i+j][1:-1].split(',')]) \n",
    "            \n",
    "            # tAnt = t[ini]\n",
    "            # tTaken = []\n",
    "            \n",
    "            # j=ini\n",
    "            # while j < ini+pas & len(tTaken) < N10s:    # Fill a 10 seconds Frame, skipping old data\n",
    "             \n",
    "            #     cvv=[]\n",
    "                \n",
    "            #     #while t[j] < tAnt: # move on, if first sensor is older than tAnt\n",
    "            #     #    j+=NS\n",
    "            #     tTaken.append(j)\n",
    "            #     print('tTaken',tTaken)\n",
    "                \n",
    "            #     # vetTaken=[]\n",
    "            #     # aux=ini\n",
    "            #     # while aux < j:\n",
    "            #     #     vetTaken.append(t[tTaken[aux]])\n",
    "            #     #     aux+=NS\n",
    "            #     # print('t[tTaken]',vetTaken)\n",
    "                \n",
    "            #    for i in range(NS):            \n",
    "            #        cvv.append([float(item) for item in y[i+j][1:-1].split(',')]) \n",
    "\n",
    "\n",
    "                cv=np.array(cvv)\n",
    "                cv=cv.flatten()            # 1080 CSI values => 1 line of CSI image\n",
    "\n",
    "                ci=cv[0:len(cv):2]         # Frame Size = [N10s/Ns,(216/2 Im,Re || Amp,Pha)*NS]\n",
    "                cr=cv[1:len(cv):2]\n",
    "\n",
    "                if NS ==2:\n",
    "                    idxRm = [26, 53, 80, 107, 134, 161, 188, 215]     # Remove 8 beacon zeroes \n",
    "                else: # NS ==4:\n",
    "                    idxRm = [ 26,  53,  80, 107, 134, 161, 188, 215, 242, 269, 296, 323, 350, 377, 404, 431]\n",
    "    \n",
    "                #else:  # NS == 6\n",
    "                #    idxRm = [ 26,  53,  80, 107, 134, 161, 188, 215, 242, 269, 296, 323, 350, 377, 404, 431, 458, 485, 512, 539, 566, 593, 620, 647]\n",
    "     \n",
    "                for m in sorted (idxRm, reverse=True):\n",
    "                    ci=np.delete(ci,m)\n",
    "                    cr=np.delete(cr,m)\n",
    "                \n",
    "                CSi.append(ci)\n",
    "                CSr.append(cr)\n",
    "                \n",
    "                # tAnt = t[j]\n",
    "                # #if j==N10s:     # 52, 13 x 2 = 26 lines of a image fullfilled \n",
    "                # #    break\n",
    "                # j=+NS\n",
    "                \n",
    "            emptyTime=np.append(emptyTime,t[pas]-t[j])\n",
    "            \n",
    "            imgI = np.array(CSi,dtype=float)\n",
    "            imgR = np.array(CSr,dtype=float)\n",
    "\n",
    "            #aa=np.array(imgI)\n",
    "            #ap=np.array(imgR)\n",
    "            li,co = imgI.shape\n",
    "            \n",
    "            if oAbs: rang=li\n",
    "            else: rang=2*li\n",
    "            \n",
    "            if oAbs:\n",
    "                ab=np.zeros((li,co)) # both: Amp & Phase\n",
    "                for i in range(li):\n",
    "                    for c in range(co):\n",
    "                        ab[i][c]=abs(complex(imgI[i,c],imgR[i,c]))\n",
    "            else:\n",
    "                ab=np.zeros((2*li,co)) # both: Amp & Phase\n",
    "                for i in range(li):\n",
    "                    for c in range(co):\n",
    "                        ab[i][c]=abs(complex(imgI[i,c],imgR[i,c]))\n",
    "                        ab[i+li][c]=cmath.phase(complex(imgI[i,c],imgR[i,c]))\n",
    "                    \n",
    "            \n",
    "    ######################################\n",
    "    # median Filter - catch outlyers in each line\n",
    "    ######################################                   \n",
    "            #print('wFilter',wFilter)\n",
    "            if wFilter:\n",
    "                for i in range(rang):\n",
    "                    ab[i] = signal.medfilt(ab[i])    \n",
    "                \n",
    "    ######################################\n",
    "    # Sanitize Phase \n",
    "    ######################################                   \n",
    "            if oAbs == False:\n",
    "                if phaseSanit:\n",
    "                    for i in range(li-1):   # Use difference of the same carrier frequency\n",
    "                        for c in range(co):\n",
    "                            delta = ab[i+li+1][c]-ab[i+li][c]\n",
    "                            if delta > 3.1415926:\n",
    "                                ab[i+li+1][c]-=2*3.1415926\n",
    "                            if delta < -3.1415926:\n",
    "                                ab[i+li+1][c]+=2*3.1415926\n",
    "                    \n",
    "                # 11Fev2025            \n",
    "                # if np.mean(ab[li:2*li][:]) > 0: #Ensure causality, negative phase!\n",
    "                #     ab[li:2*li][:]-=2*3.1415926\n",
    "    ######################################\n",
    "    # Low Pass - reduce noise \n",
    "    ######################################                   \n",
    "            if wFilter:\n",
    "                for i in range(rang):\n",
    "                    ab[i] = butter_lowpass_filter(ab[i],cutoff,fs,3)      \n",
    "\n",
    "    # 23.Jan.2023\n",
    "    #    sit stand walk empty empty walk stand sit stand walk empty empty walk stand sit\n",
    "    # k: 0   3     6    9     12    15   18    21  24    27   30    33    36   39    42\n",
    "            \n",
    "    # 17 Set 2024 LARA Leandro & Andressa\n",
    "    #    empty  walk  sit  stand walk \n",
    "    # k: 0   6     9   15     21   24\n",
    "\n",
    "            if k in range(0,6): ttxt='empty'\n",
    "            if k in range(6,9) or k in range (21,24): ttxt='walk'\n",
    "            if k in range(9,15): ttxt='sit'\n",
    "            if k in range(15,21): ttxt='stand'\n",
    "\n",
    "            if showIm:\n",
    "                if ex==0:\n",
    "                    axs[k//6,k%6].imshow(ab, interpolation='nearest',aspect='auto')   \n",
    "                    axs[k//6,k%6].axis('off')\n",
    "                    axs[k//6,k%6].title.set_text(ttxt)\n",
    "\n",
    "            if wPCA:\n",
    "                abPCA = pca.fit_transform(ab)\n",
    "            else:\n",
    "                abPCA = ab\n",
    "            \n",
    "            if showIm:\n",
    "                if ex==0:\n",
    "                    ax[k//6,k%6].imshow(abPCA, interpolation='nearest',aspect='auto')   \n",
    "                    ax[k//6,k%6].axis('off')\n",
    "                    ax[k//6,k%6].title.set_text(ttxt)\n",
    "                    \n",
    "\n",
    "            trainy.append(ttxt)\n",
    "            trainx.append(abPCA)    \n",
    "                  \n",
    "        tt=np.array(t)\n",
    "\n",
    "        #-----------------------\n",
    "        if pltCSItiming:\n",
    "            if ex==0:\n",
    "                plt.plot(tt[0:52*3]*0.001,label='t')\n",
    "                for indi in range(3):\n",
    "                    plt.plot([0, 52*3],[indi*10, indi*10],'--')\n",
    "                plt.title('t in '+file_in)\n",
    "                plt.legend()\n",
    "                plt.xlabel('CSI line in csv File')\n",
    "                plt.ylabel('s')\n",
    "                plt.show()\n",
    "        \n",
    "    \n",
    "        emptyTime=emptyTime[:-1]\n",
    "    \n",
    "    trainy = np.array(trainy)\n",
    "    if wEmptyFrames:\n",
    "        plt.figure()\n",
    "        plt.plot(emptyTime[:-1]/1000,label='et')\n",
    "        plt.title('Empty Time in Frames of '+file_in)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Frames')\n",
    "        plt.ylabel('s')\n",
    "        plt.show()\n",
    "        \n",
    "    return trainx, trainy, imgI, imgR, ab, abPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y, imgI, imgR, ab, abPCA = read_csiKIT(trainD=False,wPCA=wPCA, showIm=showIm,Niter=NTest,wFilter=wFilter,oAbs=oAbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('ab',ab.shape)\n",
    "# if oAbs:\n",
    "#     pca = PCA(n_components = B10s)   # only Abs\n",
    "# else: \n",
    "#     pca = PCA(n_components = 2*B10s)   # only Abs\n",
    "# abPCA = pca.fit_transform(ab)\n",
    "# print('abPCA',abPCA.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_sample(n_way, n_support, n_query, datax, datay, test = False):\n",
    "  \"\"\"\n",
    "  Picks random sample of size n_support+n_querry, for n_way classes\n",
    "  Args:\n",
    "      n_way (int): number of classes in a classification task\n",
    "      n_support (int): number of labeled examples per class in the support set\n",
    "      n_query (int): number of labeled examples per class in the query set\n",
    "      datax (np.array): dataset of CSI dataframes\n",
    "      datay (np.array): dataset of labels\n",
    "  Returns:\n",
    "      (dict) of:\n",
    "        (torch.Tensor): sample of CSI dataframes. Size (n_way, n_support+n_query, (dim))\n",
    "        (int): n_way\n",
    "        (int): n_support\n",
    "        (int): n_query\n",
    "  \"\"\"\n",
    "  sample = []\n",
    "  if test:\n",
    "  #    K = np.array(['empty', 'jump', 'stand', 'walk'])  # Bahadori\n",
    "  #    K = np.array(['empty', 'sit', 'stand', 'walk'])   # AB - Karlsruhe 2023\n",
    "      K = np.array(['empty', 'walk', 'sit', 'stand'])   # 4/2/2025\n",
    "  else:\n",
    "      K = np.random.choice(np.unique(datay), n_way, replace=False)\n",
    "  #print('K',K)\n",
    "  for cls in K:\n",
    "    datax_cls = datax[datay == cls]\n",
    "    \n",
    "    perm = np.random.permutation(datax_cls)\n",
    "    sample_cls = perm[:(n_support+n_query)]\n",
    "    sample.append(sample_cls)\n",
    "    \n",
    "  sample = np.array(sample)\n",
    "  #print('sample',sample.shape)\n",
    "  \n",
    "  sample = torch.from_numpy(sample).float()    # 10 fev 2023\n",
    "\n",
    "  return({\n",
    "      'csi_mats': sample,\n",
    "      'n_way': n_way,\n",
    "      'n_support': n_support,\n",
    "      'n_query': n_query\n",
    "      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ample_example = extract_sample(2, 8, 5, trainx, trainy)<br>\n",
    "%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Flatten, self).__init__()\n",
    "  def forward(self, x):\n",
    "    return x.view(x.size(0), -1)\n",
    "\n",
    "def euclidean_dist(x, y):\n",
    "  \"\"\"\n",
    "  Computes euclidean distance btw x and y\n",
    "  Args:\n",
    "      x (torch.Tensor): shape (n, d). n usually n_way*n_query\n",
    "      y (torch.Tensor): shape (m, d). m usually n_way\n",
    "  Returns:\n",
    "      torch.Tensor: shape(n, m). For each query, the distances to each centroid\n",
    "  \"\"\"\n",
    "  n = x.size(0)\n",
    "  m = y.size(0)\n",
    "  d = x.size(1)\n",
    "  assert d == y.size(1)\n",
    "\n",
    "  x = x.unsqueeze(1).expand(n, m, d)\n",
    "  y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "  return torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "  def __init__(self, encoder):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        encoder : CNN encoding the CSI dataframes in sample\n",
    "        n_way (int): number of classes in a classification task\n",
    "        n_support (int): number of labeled examples per class in the support set\n",
    "        n_query (int): number of labeled examples per class in the query set\n",
    "    \"\"\"\n",
    "    super(ProtoNet, self).__init__()\n",
    "   # self.encoder = encoder.cuda(0)\n",
    "    self.encoder = encoder.to(device)\n",
    "  def set_forward_loss(self, sample):\n",
    "    \"\"\"\n",
    "    Computes loss, accuracy and output for classification task\n",
    "    Args:\n",
    "        sample (torch.Tensor): shape (n_way, n_support+n_query, (dim)) \n",
    "    Returns:\n",
    "        torch.Tensor: shape(2), loss, accuracy and y_hat\n",
    "    \"\"\"\n",
    "   # sample_images = sample['csi_mats'].cuda(0)\n",
    "    sample_images = sample['csi_mats'].to(device)\n",
    "    n_way = sample['n_way']\n",
    "    n_support = sample['n_support']\n",
    "    n_query = sample['n_query']\n",
    "    x_support = sample_images[:, :n_support]\n",
    "    x_query = sample_images[:, n_support:]\n",
    "   \n",
    "    #target indices are 0 ... n_way-1\n",
    "    target_inds = torch.arange(0, n_way).view(n_way, 1, 1).expand(n_way, n_query, 1).long()\n",
    "    target_inds = Variable(target_inds, requires_grad=False)\n",
    "   # target_inds = target_inds.cuda(0)\n",
    "    target_inds = target_inds.to(device)\n",
    " \n",
    "    #encode CSI dataframes of the support and the query set\n",
    "    x = torch.cat([x_support.contiguous().view(n_way * n_support, *x_support.size()[2:]),\n",
    "                   x_query.contiguous().view(n_way * n_query, *x_query.size()[2:])], 0)\n",
    "   \n",
    "    z = self.encoder.forward(x)\n",
    "    z_dim = z.size(-1) #usually 64\n",
    "    z_proto = z[:n_way*n_support].view(n_way, n_support, z_dim).mean(1)\n",
    "    z_query = z[n_way*n_support:]\n",
    "\n",
    "    #compute distances\n",
    "    dists = euclidean_dist(z_query, z_proto)\n",
    "    \n",
    "    #compute probabilities\n",
    "    log_p_y = F.log_softmax(-dists, dim=1).view(n_way, n_query, -1)\n",
    "   \n",
    "    loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "    _, y_hat = log_p_y.max(2)\n",
    "    acc_val = torch.eq(y_hat, target_inds.squeeze()).float().mean()\n",
    "   \n",
    "    return loss_val, {\n",
    "        'loss': loss_val.item(),\n",
    "        'acc': acc_val.item(),\n",
    "        'y_hat': y_hat\n",
    "        # ,'target':target\n",
    "        }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_protonet_conv(**kwargs):\n",
    "  \"\"\"\n",
    "  Loads the prototypical network model\n",
    "  Arg:\n",
    "      x_dim (tuple): dimension of input CSI dataframes\n",
    "      hid_dim (int): dimension of hidden layers in conv blocks\n",
    "      z_dim (int): dimension of embedded CSI dataframes\n",
    "  Returns:\n",
    "      Model (Class ProtoNet)\n",
    "  \"\"\"\n",
    "  x_dim = kwargs['x_dim']\n",
    "  hid_dim = kwargs['hid_dim']\n",
    "  z_dim = kwargs['z_dim']\n",
    "\n",
    "  print(\"x_dim\", x_dim)\n",
    "  print(\"hid_dim\", hid_dim)\n",
    "  print(\"z_dim\", z_dim)\n",
    "\n",
    "  def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "        )\n",
    "  if CNN_Leandro:\n",
    "    encoder = nn.Sequential(\n",
    "      conv_block(x_dim[0], hid_dim),\n",
    "      conv_block(hid_dim, hid_dim),\n",
    "      conv_block(hid_dim, z_dim),\n",
    "      Flatten()\n",
    "      )\n",
    "  else:  \n",
    "    encoder = nn.Sequential(\n",
    "      conv_block(x_dim[0], hid_dim),\n",
    "      conv_block(hid_dim, hid_dim),\n",
    "      conv_block(hid_dim, hid_dim),\n",
    "      conv_block(hid_dim, z_dim),\n",
    "      Flatten()\n",
    "      )\n",
    "\n",
    "  return ProtoNet(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_x, train_y, n_way, n_support, n_query, max_epoch, epoch_size):\n",
    "  \"\"\"\n",
    "  Trains the protonet\n",
    "  Args:\n",
    "      model\n",
    "      optimizer\n",
    "      train_x (np.array): CSI dataframes of training set\n",
    "      train_y(np.array): labels of training set\n",
    "      n_way (int): number of classes in a classification task\n",
    "      n_support (int): number of labeled examples per class in the support set\n",
    "      n_query (int): number of labeled examples per class in the query set\n",
    "      max_epoch (int): max epochs to train on\n",
    "      epoch_size (int): episodes per epoch\n",
    "  \"\"\"\n",
    "  #divide the learning rate by 2 at each epoch, as suggested in paper\n",
    "  scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "  epoch = 0 #epochs done so far\n",
    "  stop = False #status to know when to stop\n",
    "\n",
    "  while epoch < max_epoch and not stop:\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    for episode in trange(epoch_size, desc=\"Epoch {:d} train\".format(epoch+1)):\n",
    "      sample = extract_sample(n_way, n_support, n_query, train_x, train_y, test = False)\n",
    "      #print(sample)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss, output = model.set_forward_loss(sample)\n",
    "      running_loss += output['loss']\n",
    "      running_acc += output['acc']\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    epoch_loss = running_loss / epoch_size\n",
    "    epoch_acc = running_acc / epoch_size\n",
    "    print('Epoch {:d} -- Loss: {:.4f} Acc: {:.4f}'.format(epoch+1,epoch_loss, epoch_acc))\n",
    "    \n",
    "    epoch += 1\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_dim (1, 64, 64)\n",
      "hid_dim 128\n",
      "z_dim 64\n",
      "ProtoNet(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Flatten()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# args.device = torch.device('cuda:' + str(args.gpu_id))\n",
    "\n",
    "#model = load_protonet_conv(\n",
    "#    x_dim=(1,512,256),\n",
    "#    hid_dim=64,\n",
    "#    z_dim=64,\n",
    "#    )\n",
    "\n",
    "\n",
    "#model = load_protonet_conv(\n",
    "#    x_dim=(1,242,242),\n",
    "#    x_dim=(1,208,208),\n",
    "#    x_dim=(1,34,34),\n",
    "#    x_dim=(1,32,32), # 21/10/2024\n",
    "#     x_dim=(1,64,64),\n",
    "#    hid_dim=32,\n",
    "#    hid_dim=128,\n",
    "#    z_dim=64,\n",
    "#    )    \n",
    "\n",
    "if CNN_Leandro:\n",
    "    model = load_protonet_conv(\n",
    "        x_dim=(1,64,64),\n",
    "        hid_dim=128,\n",
    "        z_dim=64,\n",
    "        )    \n",
    "else:\n",
    "    model = load_protonet_conv(\n",
    "        x_dim=(1,512,512),\n",
    "        hid_dim=248,\n",
    "        z_dim=64,\n",
    "        )    \n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, test_x, test_y, n_way, n_support, n_query, test_episode):\n",
    "  \"\"\"\n",
    "  Tests the protonet\n",
    "  Args:\n",
    "      model: trained model\n",
    "      test_x (np.array): CSI dataframes of testing set\n",
    "      test_y (np.array): labels of testing set\n",
    "      n_way (int): number of classes in a classification task\n",
    "      n_support (int): number of labeled examples per class in the support set\n",
    "      n_query (int): number of labeled examples per class in the query set\n",
    "      test_episode (int): number of episodes to test on\n",
    "  \"\"\"\n",
    "  conf_mat = torch.zeros(n_way, n_way)\n",
    "  running_loss = 0.0\n",
    "  running_acc = 0.0\n",
    "  for episode in trange(test_episode):\n",
    "    sample = extract_sample(n_way, n_support, n_query, test_x, test_y, test = True)\n",
    "    #print('test_episode',test_episode,'test_y: ',test_y,'sample: ',sample)\n",
    "    loss, output = model.set_forward_loss(sample)\n",
    "    a = output['y_hat'].cpu().int()\n",
    "\n",
    "    #print('sample n_way,n_support,n_query',sample['n_way'], sample['n_support'],sample['n_query'])\n",
    "    #print('sample CSI',sample['csi_mats'])\n",
    "      # 'csi_mats': sample,\n",
    "      # 'n_way': n_way,\n",
    "      # 'n_support': n_support,\n",
    "      # 'n_query': n_query\n",
    "      # })\n",
    "    #print('CNN output: ',a)\n",
    "\n",
    "    #a = output['y_hat'].mps().int()\n",
    "    for cls in range(n_way):\n",
    "        conf_mat[cls,:] = conf_mat[cls,:] + torch.bincount(a[cls,:], minlength=n_way) \n",
    "    \n",
    "    # To check accuracy!\n",
    "    #print('torch.bincount(a[cls,:]',torch.bincount(a[cls,:]))\n",
    "        \n",
    "    running_loss += output['loss']\n",
    "    running_acc += output['acc']\n",
    "  avg_loss = running_loss / test_episode\n",
    "  avg_acc = running_acc / test_episode\n",
    "  print('Test results -- Loss: {:.4f} Acc: {:.4f}'.format(avg_loss, avg_acc))\n",
    "  return (conf_mat/(test_episode*n_query), avg_acc)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# CNN -> Study Room\n",
    "###########################################        trainD=False => TEST!\n",
    "# val_x, val_y, imgI, imgR, ab, abPCA = read_csiKIT(trainD=True,wPCA=wPCA, showIm=showIm,Niter=NTest,wFilter=wFilter,oAbs=oAbs)\n",
    "\n",
    "# val_x = np.array(val_x)\n",
    "# val_x = np.expand_dims(val_x, axis=1)\n",
    "# val_y = np.array(val_y)\n",
    "\n",
    "# train(model, optimizer, val_x, val_y, n_way, n_support, n_query, max_epoch, epoch_size)\n",
    "\n",
    "# test_episode = 100\n",
    "# CF_CNN, acc_CNN = test(model, val_x, val_y, n_way, n_support, n_query, test_episode)\n",
    "\n",
    "# resultCNN = {'CF_P':CF_P, 'acc_P':acc_P,'CF_CNN':CF_CNN, 'acc_CNN':acc_CNN}\n",
    "# print('\\n>>> (Meeting Room) acc_CNN('+model_out_name[-15:-3]+'): ',\n",
    "#       'train_x+test_x acc_P:',round(acc_P,3),\n",
    "#       'all(TrainD) acc_CNN:',round(acc_CNN,3),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 1, 28, 28) (371,) (66, 1, 28, 28) (66,) TRAIN prototypes split 0.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb59f5d623604580b84ff57a6bae95f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Loss: 0.7462 Acc: 0.7186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c506d30afa1041838b714a113d8900fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 -- Loss: 0.0547 Acc: 0.9838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41f9115c97f4741bd8b095eea3ed559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results -- Loss: 2.1542 Acc: 0.4917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db8f68704b64295a570a62debd368c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results -- Loss: 1.4939 Acc: 0.5175\n",
      "\n",
      ">>> Este treino B10s14_wF_wS_wPCA_n4sup_n3qry acc_P 0.4916666616499424  acc_FS  0.5174999955296516\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Save anyway if it is the first training for this model+meta-parameters\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(model_out_name):\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_out_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# torch.save({\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m#     'model_state_dict': model.state_dict(),\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m#     'optimizer_state_dict': optimizer.state_dict(),\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m#     }, model_out_name)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(result, result_out_name)\n",
      "File \u001b[1;32mc:\\Users\\Dekomonte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dekomonte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dekomonte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "########################\n",
    "# # MAIN TEST LOOP\n",
    "########################\n",
    "# q=[2,3,4,5,6,7,8]\n",
    "# for qry in range q:\n",
    "\n",
    "if not useBest: # new model trained - create Prototypes from trainD\n",
    "    #################################\n",
    "    # Load TRAIN Data -> study Room\n",
    "    # # ===================================================>  TRAIN\n",
    "    trainx, trainy, imgI, imgR, ab, abPCA = read_csiKIT(trainD=True,wPCA=wPCA, showIm=showIm,Niter=NTrain,wFilter=wFilter,oAbs=oAbs)\n",
    "\n",
    "    trainx=np.array(trainx)\n",
    "    trainx = np.expand_dims(trainx, axis=1)\n",
    "\n",
    "    ############\n",
    "    # Split - Train + Test \n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.003)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(trainx, trainy, test_size = 0.15, random_state=10, shuffle=True)\n",
    "\n",
    "    train_y=np.array(train_y)\n",
    "    test_y=np.array(test_y)\n",
    "    print(train_x.shape,train_y.shape,test_x.shape,test_y.shape,'TRAIN prototypes split 0.15')\n",
    "    trainy=np.array(trainy)\n",
    "\n",
    "    #########################################################\n",
    "    # Train embedding to get prototypes with train_x, train_y\n",
    "    ########################################################\n",
    "    train(model, optimizer, train_x, train_y, n_way, n_support, n_query, max_epoch, epoch_size)\n",
    "    \n",
    "    # Test with a different set test_x, testy\n",
    "    CF_P, acc_P = test(model, test_x, test_y, n_way, n_support, n_query, test_episode)\n",
    "\n",
    "if useBest:\n",
    "    if os.path.isfile(model_out_name):       \n",
    "        model=torch.load(model_out_name, weights_only=False)\n",
    "    \n",
    "###########################################\n",
    "# fewShot Transfer Learning -> Meeting Room\n",
    "###########################################        trainD=False => TEST!\n",
    "val_x, val_y, imgI, imgR, ab, abPCA = read_csiKIT(trainD=False,wPCA=wPCA, showIm=showIm,Niter=NTest,wFilter=wFilter,oAbs=oAbs)\n",
    "\n",
    "val_x = np.array(val_x)\n",
    "val_x = np.expand_dims(val_x, axis=1)\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "# >>>>>>>>>\n",
    "# train(model, optimizer, val_x, val_y, n_way, n_support, n_query, max_epoch, epoch_size)\n",
    "# >>>>>>>>>\n",
    "\n",
    "CF_FS, acc_FS = test(model, val_x, val_y, n_way, n_support, n_query, test_episode)\n",
    "result = {'CF_P':CF_P, 'acc_P':acc_P,'CF_FS':CF_FS, 'acc_FS':acc_FS}\n",
    "print('\\n>>> Este treino '+ nameFile[:-3]+' acc_P',acc_P,' acc_FS ',acc_FS)\n",
    "\n",
    "\n",
    "# Save anyway if it is the first training for this model+meta-parameters\n",
    "if not os.path.isfile(model_out_name):\n",
    "    torch.save(model, model_out_name)\n",
    "    # torch.save({\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     }, model_out_name)\n",
    "    torch.save(result, result_out_name)\n",
    "    torch.save(meta, meta_out_name)\n",
    "\n",
    "shouldSave=True\n",
    "if os.path.isfile(result_out_name):\n",
    "    SavedResult = torch.load(result_out_name)\n",
    "    if SavedResult['acc_FS'] > acc_FS:\n",
    "        shouldSave=False\n",
    "        print('>>> Modelo salvo ainda é melhor - será mantido! acc_FS:',\n",
    "             round(acc_FS,3),'saved ',round(SavedResult['acc_FS'],3),'\\n')\n",
    "if shouldSave:\n",
    "    torch.save(model, model_out_name)\n",
    "    torch.save(result, result_out_name)\n",
    "    torch.save(meta, meta_out_name)\n",
    "    print('>>> Salvando este modelo - é o melhor FS!\\n')                                                                                \n",
    "\n",
    "#############################\n",
    "# Sorted printout by DeepSeek\n",
    "#############################\n",
    "dictRes= {'acc':[],'model':[]}\n",
    "\n",
    "bestResults = glob.glob('results/result*.pt')\n",
    "for i in range(len(bestResults)):\n",
    "    result=torch.load(bestResults[i])\n",
    "    ttx = str(bestResults[i])\n",
    "    ttx=ttx[15:-3]\n",
    "    dictRes['model'].append(ttx)\n",
    "    dictRes['acc'].append(round(float(result['acc_FS']),3))\n",
    "    \n",
    "# Pair and sort by accuracy in descending order\n",
    "paired = list(zip(dictRes['acc'], dictRes['model']))\n",
    "sorted_pairs = sorted(paired, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "for acc, model in sorted_pairs:\n",
    "    print(f\"acc({model}):\\t {acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
